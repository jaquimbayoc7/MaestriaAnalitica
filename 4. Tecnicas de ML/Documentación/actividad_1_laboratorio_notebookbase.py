# -*- coding: utf-8 -*-
"""Actividad 1_Laboratorio_NotebookBase.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16fZxOBN-GhvVnJuWmcmMe2OU2dUqJZQc

<img src="https://www.unir.net/wp-content/uploads/2019/11/Unir_2021_logo.svg" width="240" height="240" align="right"/>

<center><h1>Técnicas de Inteligencia Artificial</header1></center>
<left><h1>Actividad 1. Laboratorio: Árboles de decisión, reglas y ensemble learning</header1></left>

Presentado por: Julian Andres Quimbayo Castro  <br>
Fecha: 26/12/2022

## Importación de librerias necesarias
"""

#Para esta actividad se importarán las siguientes librerías:
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Librerías scikit-learn
from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree
from sklearn.metrics import accuracy_score
from sklearn import tree
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import plot_confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import plot_roc_curve
!pip install scikit-plot
from scikitplot.metrics import plot_roc
from scikitplot.metrics import plot_precision_recall

"""## Cargar el Dataset"""

#Código para cargar el Dataset
url = 'https://raw.githubusercontent.com/oscar-unir/TIA/main/datasets/car/Laboratorio_dataset_car.csv'

datos = pd.read_csv(url, sep=';')

datos.head()

"""## Explique el problema a resolver. 
Descripción del problema. Tipo de problema (justifique). Variable objetivo, variables de entrada. Utilidad de su posible solución. Elementos adicionales que considere relevantes:

El dataset es acerca de datos correspondientes a evaluación de aspectos de carros con respecto a su nivel de seguridad, las instancias son 1750 en total. Como variables de entrada existen Buying, manintenance, Doors, person, lug_boot, class y la variable objetivo es safety para este caso en particular. Safety posee 3 valores para las instancias las cuales son: Low(bajo), med(Medio) y High(Alto).

Variables de entrada:

buying: vhigh, high, med, low.
maint: vhigh, high, med, low.
doors: 2, 3, 4, 5more.
persons: 2, 4, more.
Class: unacc, acc, good, vgood
lug_boot: small, med, big.

Variable Objetivo:
safety: low, med, high.

El problema es determinar ¿cuál es la clase con mejor probabilidad de clasificación para las tres clases usando arboles de decisión y random forest? con esto se podría clasificar la seguridad de autos nuevos con base en los diferentes algoritmos.

## Caracterización del Dataset

Se incluye una descripción de los datos con:

>- Número de clases de la variable objetivo, indicando que representan dichas clases y el tipo de valor que toman.
>- Número de instancias en total.
>- Número de instancias pertenecientes a cada clase.
>- Número de atributos de entrada, su significado y tipo.
>- ¿Hay algún valor de atributo desconocido?

Se incorporá una descripción (EDA) del conjunto de datos utilizado. Se analiza el dataset proporcionando, se muestra al menos algunas de sus características mediante tablas y al menos algunas de ellas en modo gráfico (p.ej., histogramas, diagramas de dispersión, diagramas de cajas y bigotes, etc.)
"""

#Código que responde a la descripción anterior - Funciones particulares
# Función informe sobre los datos NAN e información general
def datos_NA(df):
    if isinstance(df, pd.DataFrame):
        total_na = df.isna().sum().sum()
        print("Dimensiones : %d filas, %d columnas" %
              (df.shape[0], df.shape[1]))
        print("Total Valores NA : %d " % (total_na))
        print("%38s %10s     %10s %10s" %
              ("Nombre Columna", "Tipo de Dato", "#Distintos", "Valores NA"))
        col_name = df.columns
        dtyp = df.dtypes
        uniq = df.nunique()
        na_val = df.isna().sum()
        for i in range(len(df.columns)):
            print("%38s %10s   %10s %10s" %
                  (col_name[i], dtyp[i], uniq[i], na_val[i]))

    else:
        print("Se esperaba dataframe %15s" % (type(df)))

##Función para Número de instancias por clase
def instClase(df):
      for i in df.columns:
            print(df[i].value_counts())

##Función para diagrama de caja
def diagCaja(df, coluno, coldos):
      sns.boxplot(x =coluno,
             y =coldos,
             data = df).set_title("Diagrama de Caja")

##Función para diagrama de barras
def diagBarra(df, coluno, coldos):
      sns.countplot(data=df, x=coluno, hue=coldos)

##Función para partición de variables entrada y objetivo
def particion(df, col):
      X = df.drop([col], axis=1)
      y = df[col]
      return X, y

##Función para one hot encoding
def oneHot(df):
      return pd.get_dummies(df)

datos_NA(datos)
##Se evidencian tres clases de la variable objetivo safety, las cuales determinan el nivel de seguridad del auto según la evaluación
##low (Bajo), Med(Medio), High(Alto)
##1750 instancias en total por 7 columnas, todas las variables son de tipo Object por tal motivo se debe proceder a un encoding para
##trabajar el algoritmo de mejor manera.

##Cantidad de instancias por clase
instClase(datos)
##Para la clase de compra en cuanto al precio(Buying) la distribución es relativamente equitativa entre vhigh(443), med(438), low(437) 
# y high(432); de igual forma mantenimiento (maintenance) posee la mismas categorías vhigh(437), med(434), low(447) y high(432). Para el
#número de puertas(Doors) las categorías son:2(444), 5more(437), 3(435) y 4(434), para cantidad de personas u ocupación del auto(Person)
#la distribución es, 2(578), 4(587) y more(585). En cuanto a capacidad del maletero(lug_boot) la distribución es big(585), med(583), small(582).
#En cuanto a seguridad(safety) su distribución es, high(590), med(582), low(578), cuenta con un buen balance para el algoritmo de clasificación.
#Finalmente, la variable class posee 4 categorias las cuales son, unacc(1215), acc(390), good(75) y vgood(70).

##Todos los valores son conocidos, no existen datos NAN aleatorios, ni tipográficos.

#Código que responde a la descripción anterior (incorpore las lineas de code necesarias. Describa cadas sentencia de código)
#Visualización de la información
diagBarra(datos, 'safety','class')
#La mayor cantidad de autos categorizados como no accesibles llega casi a las 600 observaciones y se categoriza como seguridad baja.
#a medida que aumenta el nivel de seguridad asi mismo disminuye la relación de no accesible. la distribución de autos muy bueno o buenos
#en cuanto a seguridad es demasiado bajo. Se debe observar al final de realizar el algoritmo si es mas considerable balancear el dataset 
#o si es suficiente con la variable objetivo safety que presenta buena distribución.

##En cuanto a distribución de la variable safety vs el tamaño del maletero se presenta uniformidad, es determinante la variable class,
##no tanto las demas para construir las reglas del arbol para tener atención en la creación del modelo.
diagBarra(datos, 'safety','lug_boot')

"""En un par de párrafos haga un resumen de los principales hallazagos encontrados: se evidencia una relación clave entre safety y class, normalmente el criterio de variable objetivo debería ser class con sus niveles unacc, acc, vgood y good pero para este caso en particular se desea clasificar por medio del nivel de safety low,med y high. Se debe revisar como afecta el balanceo del dataset de manera positiva o negativa la clasificación y en caso de ser necesario nivelar la data. En cuanto a las demas variables presentan una distribución uniforme y se debe utilizar un enconding con el fin de mejorar el performance se sugiere variables dummies y/o one hot encoding.

## Preprocesamiento del dataset. Transformaciones previas necesarias para la modelación
"""

#Código que realice las transformaciones necesarias para poder realizar los procesos de modelación. Ej.One hot enconding
#Partición de los datos en variables de entrada y variable objetivo
X, y = particion(datos, 'safety')
X.head, y.head

##One hot encoding a variables de Entrada X
X = oneHot(X)
X.head()

"""## División del dataset en datos de entrenamiento y datos de test """

#Código que realice la división en entrenamiento y test, de acuerdo con la estretgia de evluación planeada. Describa cuál es.
#se realiza una partición inicial de 90% datos de entrenamiento y 10% de datos de testeo y un random state de 100.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 100)

"""## Ajuste de los modelos de clasificación propuestos

Justifique la selección de las dos propuestas de modelación seleccionadas:
"""

#Código de ajuste del modelo de clasificación 1 - Parámetro Gini
algoGini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
algoGini.fit(X_train, y_train)
ypredgini = algoGini.predict(X_test)
print("Valor de eficiencia: {:.3f}".format(round(accuracy_score(y_test, ypredgini),2)))

##Revisión de overfitting y underfitting
algoGini.score(X_train, y_train)

algoGini.score(X_test, y_test)
#el valor de overfitting y underfitting es relativamente bajo sobre el 0.04 lo cual es aceptable.

##Variables importantes
variables = pd.DataFrame(index=X.columns,data=algoGini.feature_importances_,columns=['Caracteristicas importantes'])
variables[variables['Caracteristicas importantes']>=0.1]

plt.figure(figsize=(8,8),dpi=100)
plot_tree(algoGini,filled=True,feature_names=X.columns)

plot_confusion_matrix(algoGini, X_test, y_test)

##Matriz de clasificación - Gini
print(classification_report(y_test, ypredgini))

#Código de ajuste del modelo de clasificación 2 - Parámetro Entropy

algoEntropy = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
algoEntropy.fit(X_train, y_train)
ypredEntropy = algoEntropy.predict(X_test)
print("Valor de eficiencia: {:.3f}".format(round(accuracy_score(y_test, ypredgini),2)))

##Revisión de overfitting y underfitting
algoEntropy.score(X_train, y_train)

algoEntropy.score(X_test, y_test)
#el valor de overfitting y underfitting es relativamente bajo sobre el 0.04 lo cual es aceptable.

##Variables importantes
variables = pd.DataFrame(index=X.columns,data=algoEntropy.feature_importances_,columns=['Caracteristicas importantes'])
variables[variables['Caracteristicas importantes']>=0.1]

plt.figure(figsize=(8,8),dpi=100)
plot_tree(algoEntropy,filled=True,feature_names=X.columns)

plot_confusion_matrix(algoEntropy, X_test, y_test)

##Matriz de clasificación - ENtropy
print(classification_report(y_test, ypredEntropy))

##Modelo random forest Clasificación 3
rfc=RandomForestClassifier(random_state=42)

param_grid = { 
    'n_estimators': [200, 500],
    'max_features': ['sqrt'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy']
}

GridRF = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
GridRF.fit(X_train, y_train)

GridRF.best_params_

RFMODEL=RandomForestClassifier(random_state=42, max_features='sqrt', n_estimators= 200, max_depth=4, criterion='entropy')

RFMODEL.fit(X_train, y_train)

y_pred_RF=RFMODEL.predict(X_test)

"""## Evaluación de cada modelo

Al menos incluya:

+ Instancias clasificadas correctamente
+ Instancias clasificadas incorrectamente
+ TP Rate
+ FP Rate
+ Matriz de confusión

"""

#Código para mostrar la evaluación del modelo de clasificación 1
##Matriz de clasificación - Gini
print(classification_report(y_test, ypredgini))
##Instancias clasificadas correctamente tenemos el recall para la clase high con 0.43, low con 0.78 y med con 0.46, lo cual quiere decir
##que el modelo es muy bueno para clasificar cuando la clase es low.

from sklearn.metrics import confusion_matrix

#High
TPH = 27
FNH = 10+26
FPH = 9+13
TNH = 47+4+15+24
print("\033[1m-----------------------------\033[0m")
print('Instancias correctas para la clase High: {} de {}'.format(TPH+TNH, y_test.shape))
print('Instancias incorrectas para la clase High: {} de {}'.format(FNH+FPH, y_test.shape))
print('TP Rate para High: {}'.format(round(TPH/(TPH+FNH),2)))
print('TN Rate para High: {}'.format(round(TNH/(TNH+FPH),2)))
print('Precision para High: {}'.format(round(TPH/(TPH+FPH),2)))
print("\033[1m-----------------------------\033[0m")
#Low
TPL = 47
FNL = 9+4
FPL = 10+15
TNL = 27+26+13+24
print("\033[1m-----------------------------\033[0m")
print('Instancias correctas para la clase Low: {} de {}'.format(TPL+TNL, y_test.shape))
print('Instancias incorrectas para la clase Low: {} de {}'.format(FNL+FPL, y_test.shape))
print('TP Rate para Low: {}'.format(round(TPL/(TPL+FNL),2)))
print('TN Rate para Low: {}'.format(round(TNL/(TNL+FPL),2)))
print('Precision para Low: {}'.format(round(TPL/(TPL+FPL),2)))
print("\033[1m-----------------------------\033[0m")
#Med
TPM = 24
FNM = 13+15
FPM = 4+26
TNM = 27+10+9+47
print("\033[1m-----------------------------\033[0m")
print('Instancias correctas para la clase Med: {} de {}'.format(TPM+TNM, y_test.shape))
print('Instancias incorrectas para la clase Med: {} de {}'.format(FNM+FPM, y_test.shape))
print('TP Rate para Med: {}'.format(round(TPM/(TPM+FNM),2)))
print('TN Rate para Med: {}'.format(round(TNM/(TNM+FPM),2)))
print('Precision para Med: {}'.format(round(TPM/(TPM+FPM),2)))
print("\033[1m-----------------------------\033[0m")

cm = confusion_matrix(y_test, ypredgini)
cm

"""Construya un párrafo con los principales hallazgos.
Se determino para la construcción del modelo dos momentos uno con el coeficiente de gini y el otro con entropy y una máxima profundidad de 3. Los datos se repartieron en 90% train y 10% test ya que con dichas configuraciones presentaba la mayor eficiencia de 0.56 si bien no es un modelo muy bueno se evidencia una alta eficiencia de clasificación de instancias correctas para la clase low con 137 de 175 y para las demas high y med con un valor de 117. En cuanto a la sensibilidad y especificidad la mejor clase para clasificar es Low con unos valores de 0.78 en ambas métricas. Tanto para las clases High como Med el True Negative posee el mejor valor, es decir el modelo es muy bueno para clasificar cuando el carro no es seguro. Esto puede ser por el tema del balanceo del dataset en la columna class. Finalmente la eficiencia del modelo en general es de 0.56 es decir para instancias futuras puede clasificar de manera correcta de 100 datos 56.
"""

#Código  para mostrarla evaluación del modelo de clasificación 2
print(classification_report(y_test, y_pred_RF))

cmRF = confusion_matrix(y_test, y_pred_RF)
cmRF

#High
TPH = 30
FNH = 27 + 6
FPH = 0+14
TNH = 60+0+29+9
print("\033[1m-----------------------------\033[0m")
print('Instancias correctas para la clase High: {} de {}'.format(TPH+TNH, y_test.shape))
print('Instancias incorrectas para la clase High: {} de {}'.format(FNH+FPH, y_test.shape))
print('TP Rate para High: {}'.format(round(TPH/(TPH+FNH),2)))
print('TN Rate para High: {}'.format(round(TNH/(TNH+FPH),2)))
print('Precision para High: {}'.format(round(TPH/(TPH+FPH),2)))
print("\033[1m-----------------------------\033[0m")
#Low
TPL = 60
FNL = 0+0
FPL = 27+29
TNL = 30+6+14+9
print("\033[1m-----------------------------\033[0m")
print('Instancias correctas para la clase Low: {} de {}'.format(TPL+TNL, y_test.shape))
print('Instancias incorrectas para la clase Low: {} de {}'.format(FNL+FPL, y_test.shape))
print('TP Rate para Low: {}'.format(round(TPL/(TPL+FNL),2)))
print('TN Rate para Low: {}'.format(round(TNL/(TNL+FPL),2)))
print('Precision para Low: {}'.format(round(TPL/(TPL+FPL),2)))
print("\033[1m-----------------------------\033[0m")
#Med
TPM = 9
FNM = 14+29
FPM = 0+6
TNM = 30+27+0+60
print("\033[1m-----------------------------\033[0m")
print('Instancias correctas para la clase Med: {} de {}'.format(TPM+TNM, y_test.shape))
print('Instancias incorrectas para la clase Med: {} de {}'.format(FNM+FPM, y_test.shape))
print('TP Rate para Med: {}'.format(round(TPM/(TPM+FNM),2)))
print('TN Rate para Med: {}'.format(round(TNM/(TNM+FPM),2)))
print('Precision para Med: {}'.format(round(TPM/(TPM+FPM),2)))
print("\033[1m-----------------------------\033[0m")

"""Construya un párrafo con los principales hallazgos.

Para la construcción del algoritmo random forest se ejecutó la validación GridSEarchCV para encontrar los mejores parámetros de ejecucción del modelo. Los datos se repartieron en 90% train y 10% test ya que con dichas configuraciones presentaba la mayor eficiencia de 0.57 si bien no es un modelo muy bueno se evidencia una alta eficiencia de clasificación de instancias correctas para la clase low con 119 de 175 y para las demas high y med con un valor de 128 y 126 respectivamente. En cuanto a la sensibilidad y especificidad la mejor clase para clasificar es Low con unos valores de 1.0. Tanto para las clases High como Med el True Negative posee el mejor valor, es decir el modelo es muy bueno para clasificar cuando el carro no es seguro.

## Comparación del desempeño de modelos
"""

#Código para mostrar la comparación de métricas de desempeño de las dos propuestas en tabla
reporteGini = classification_report(y_test, ypredgini, output_dict=True)
dfResultadosgini = pd.DataFrame(reporteGini).transpose()
dfResultadosgini

reporteRF = classification_report(y_test, y_pred_RF, output_dict=True)
dfResultadosRF = pd.DataFrame(reporteRF).transpose()
dfResultadosRF

#Código para mostrar la comparación de métricas de desempeño de las dos propuestas en gráfica - Arbol de decision Entropy
y_score_entropy = algoEntropy.predict_proba(X_test)
y_pred = algoEntropy.predict(X_test)
plot_roc(y_test, y_score_entropy)
plt.show()
    
plot_precision_recall(y_test, y_score_entropy)
plt.show()

y_score_RF = RFMODEL.predict_proba(X_test)
y_pred = RFMODEL.predict(X_test)
plot_roc(y_test, y_score_RF)
plt.show()
    
plot_precision_recall(y_test, y_score_RF)
plt.show()

"""Construya un párrafo con los principales hallazgos. Teniendo en cuenta la gráfica de rendimiento ROC del modelo en ambos modelos sea cart con Gini o Random Forest la clasificación es mucho mejor en la clase low para safety, siendo el valor de 0.87 el mejor. Por otro lado, random forest clasifica mejor la clase med con 0.70 versus un 0.66 del arbol de decisión, finalmente la clase high son muy parejas en ambos modelos. En conclusión si se desea clasificar los autos con baja y media seguridad el modelo de random forest es el mejor, si se desea clasificar el auto con seguridad alta ambos modelos son eficientes, pese al 57% de accuracy.

## Discusión de los resultados obtenidos y argumentos sobre cómo se podrían mejorar de dichos resultados

Realice en este espacio todo el análsis de resultados final incluyendo: ventajas y desventajas de cada modelo propuesto, Resultados comparados. Conclusiones objetivas y significantes con base a las diferentes métricas escogidas. Recomendaciones de mejora de las propuestas:

Como conclusiones finales se encuentra que ambos modelos son muy sensibles a sus configuraciones con los parámetros de testeo y entrenamiento al iterar la mejor recomendación es la relacion 90% entrenamiento y 10% testeo.

De igual forma se utilizó el criterio de gini y entropy para verificar la eficiencia siendo idénticas. Al no afectar se revisó el overfitting y underfitting del modelo obteniendo en ambos casos una diferencia de 0.04 puntos. Lo cual indica un modelo no sesgado, lo cual fue clave la codificación usando las variables dummies que provee pandas.

En cuanto a las variables importantes se evidencia que Person_2, class_unacc, class_vgood, son los atributos importantes que determinan la clasificación, la profundidad máxima de los arboles fue de 3 para obtener la mejor eficiencia siendo importante la librería scikit learn con poda.

Importante el uso de la técnica de hiper parametrización gridsearchCV para identificar los mejores criterios en el algoritmo random forest, siendo entropy y la profundidad de 4 árboles la mejor. Finalizando con una mejora de la clase low y med para clasificar y High siendo la menos fácil de clasificar. Ambos modelos no son lo suficiente buenos pero se puede quitar atributos para mejorar la eficiencia o revisar correlaciones despues del encoding para quitar valores que no aportan.

Finalmente, se recomienda seguir mejorando el algoritmo de randomo forest mejorando la calidad de la data para acercarse mas a una eficiencia coherente y sinergía entre sensibilidad y especificidad para poder estar equilibrado en las tres clases y ejecutar el algoritmo balanceando el atributo class.

Fuentes usadas de apoyo:

https://www.kaggle.com/code/sociopath00/random-forest-using-gridsearchcv

https://www.kaggle.com/code/nkitgupta/evaluation-metrics-for-multi-class-classification

https://www.kaggle.com/code/pythonafroz/evaluation-metrics-used-in-machine-learning
"""